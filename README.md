# 1. Multi-modal-Approach-to-Human-Robot-Interaction-using-Vision-Language-Models:

Integrating Vision model and ASR model in humanoid robot pepper
![poster_upload](https://github.com/rajkhatik24/Multi-modal-Approach-to-Human-Robot-Interaction-using-Vision-Language-Models/assets/149350286/e6f51f94-9c4e-4f40-94fd-e761c5cc1f0a)

# 2. Response from initial Prompt:

![start_chat1](https://github.com/rajkhatik24/Multi-modal-Approach-to-Human-Robot-Interaction-using-Vision-Language-Models/assets/149350286/e5f30b65-474d-48b4-8908-72c6f6ed4378)
# 3. Response from continued Prompt:

![continue_chat](https://github.com/rajkhatik24/Multi-modal-Approach-to-Human-Robot-Interaction-using-Vision-Language-Models/assets/149350286/7cc54ce9-79eb-433a-a580-1f6286a99be2)


# 4. Tracking of objects based on Prompts:

![dino_chat](https://github.com/rajkhatik24/Multi-modal-Approach-to-Human-Robot-Interaction-using-Vision-Language-Models/assets/149350286/ec0042fd-500b-4e91-af21-963070475f9e)


![tracking](https://github.com/rajkhatik24/Multi-modal-Approach-to-Human-Robot-Interaction-using-Vision-Language-Models/assets/149350286/42ae6b55-1fdf-4642-b0d2-450f4119f81f)
